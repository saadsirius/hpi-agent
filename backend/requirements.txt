fastapi==0.111.0
uvicorn==0.30.1
pydantic==2.8.2
langgraph==0.2.14
langchain==0.2.7
requests==2.32.3
# Optional if you want direct llama.cpp bindings:
# llama-cpp-python
# Optional if you want to call Ollama's HTTP server:
# (we'll just use requests)
